{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDQUmLtbd0D3"
   },
   "source": [
    "# BERT (Bidirectional Encoder Representations from Transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T14:26:04.761561Z",
     "start_time": "2024-05-17T14:25:47.021547Z"
    },
    "id": "LLVomSZqMCBD"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install pytorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATH DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\epuerta\\\\OneDrive - Universidad Tecnológica de Bolívar\\\\Apps\\\\AppsISCO\\\\InteligenciaArtificial-ISCO-A06A\\\\laboratories\\\\data\\\\'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "PATH = os.getcwd()\n",
    "DIR_DATA = PATH + '{0}data{0}'.format(os.sep)\n",
    "sys.path.append(PATH) if PATH not in list(sys.path) else None\n",
    "DIR_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Descargar recursos necesarios para el tokenizador de oraciones de NLTK\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\epuerta\\\\OneDrive - Universidad Tecnológica de Bolívar\\\\Apps\\\\AppsISCO\\\\InteligenciaArtificial-ISCO-A06A\\\\laboratories\\\\data\\\\AboutMe.txt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = DIR_DATA + 'AboutMe.txt'\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edwin Puertas is an AI Software Architect and NLP Researcher. Currently, I am an Associate Professor at the Universidad TecnolÃ³gica de BolÃ­var, and I simultaneously serve as a bridge between academia and industry in projects related to AI. My work focuses on developing innovative AI solutions that bridge the gap between theoretical research and practical applications. With over 19 years of experience in academia, I have taught courses in Artificial Intelligence, Natural Language Processing, Big Data, and Software Engineering, fostering an environment of collaborative learning and research excellence.\n",
      "\n",
      "As an AI Software Architect, I lead projects involving the design and implementation of scalable AI systems, applying advanced machine learning techniques to solve complex problems across various industries. My research in NLP explores the intricacies of human language, aiming to enhance natural language understanding and develop applications that improve human-computer interaction.\n",
      "\n",
      "I have been involved in numerous collaborative initiatives that connect academic research with industry needs, facilitating the transfer of knowledge and technology to create impactful solutions. My commitment to innovation and excellence has been recognized through various grants and awards, supporting projects that advance the field of AI. As a Senior Member of the IEEE, I actively participate in international conferences and workshops, contributing to the global discourse on AI advancements and best practices.\n"
     ]
    }
   ],
   "source": [
    "# Abre el archivo en modo lectura\n",
    "with open(filename, 'r') as archivo:\n",
    "    contexto = archivo.read()\n",
    "\n",
    "# Muestra el contenido del archivo\n",
    "print(contexto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T13:22:39.513459Z",
     "start_time": "2024-10-18T13:22:37.706419Z"
    },
    "id": "p4gGmgPMQsZh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "the_model = 'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es'\n",
    "tokenizer = AutoTokenizer.from_pretrained(the_model, do_lower_case=False)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(the_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jokmyxjASRH6",
    "outputId": "1cd9bf04-d510-45dd-9e51-d14f939c3237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForQuestionAnswering(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración 1:\n",
      "Texto original: ¿Cuál es mi nombre?\n",
      "Tokens de BERT: ['[CLS]', '¿', 'Cuál', 'es', 'mi', 'nombre', '?', '[SEP]']\n",
      "--------------------------------------------------\n",
      "Oración 2:\n",
      "Texto original: Edwin Puertas is an AI Software Architect and NLP Researcher.\n",
      "Tokens de BERT: ['[CLS]', '[UNK]', 'Puerta', '##s', 'is', 'an', 'AI', '[UNK]', 'Archi', '##tec', '##t', 'and', 'NL', '##P', 'Research', '##er', '.', '[SEP]']\n",
      "--------------------------------------------------\n",
      "Oración 3:\n",
      "Texto original: Currently, I am an Associate Professor at the Universidad TecnolÃ³gica de BolÃ­var, and I simultaneously serve as a bridge between academia and industry in projects related to AI.\n",
      "Tokens de BERT: ['[CLS]', 'Cur', '##ren', '##t', '##ly', ',', 'I', 'am', 'an', 'Associa', '##te', 'Profes', '##sor', 'at', 'the', 'Universidad', '[UNK]', 'de', '[UNK]', ',', 'and', 'I', 'simul', '##tan', '##eo', '##us', '##ly', 'serv', '##e', 'as', 'a', 'bri', '##dge', '[UNK]', 'academia', 'and', 'indust', '##ry', 'in', 'pro', '##ject', '##s', 'rela', '##ted', 'to', 'AI', '.', '[SEP]']\n",
      "--------------------------------------------------\n",
      "Oración 4:\n",
      "Texto original: My work focuses on developing innovative AI solutions that bridge the gap between theoretical research and practical applications.\n",
      "Tokens de BERT: ['[CLS]', 'My', '[UNK]', 'fo', '##cus', '##es', 'on', 'dev', '##elo', '##pin', '##g', 'innov', '##ati', '##ve', 'AI', 'solu', '##tions', 'that', 'bri', '##dge', 'the', 'ga', '##p', '[UNK]', 'the', '##ore', '##tica', '##l', 'rese', '##ar', '##ch', 'and', 'practica', '##l', 'ap', '##plica', '##tions', '.', '[SEP]']\n",
      "--------------------------------------------------\n",
      "Oración 5:\n",
      "Texto original: With over 19 years of experience in academia, I have taught courses in Artificial Intelligence, Natural Language Processing, Big Data, and Software Engineering, fostering an environment of collaborative learning and research excellence.\n",
      "Tokens de BERT: ['[CLS]', 'W', '##ith', 'over', '19', 'ye', '##ars', 'of', 'exper', '##ien', '##ce', 'in', 'academia', ',', 'I', 'have', 'ta', '##ugh', '##t', 'co', '##urs', '##es', 'in', 'Arti', '##ficial', 'Inte', '##lli', '##gen', '##ce', ',', 'Natural', 'Lang', '##ua', '##ge', 'Proces', '##sing', ',', 'Big', 'Data', ',', 'and', '[UNK]', 'Eng', '##ine', '##ering', ',', 'fos', '##teri', '##ng', 'an', 'envi', '##ron', '##ment', 'of', 'col', '##lab', '##ora', '##tive', 'lea', '##r', '##ning', 'and', 'rese', '##ar', '##ch', 'excel', '##len', '##ce', '.', '[SEP]']\n",
      "--------------------------------------------------\n",
      "Oración 6:\n",
      "Texto original: As an AI Software Architect, I lead projects involving the design and implementation of scalable AI systems, applying advanced machine learning techniques to solve complex problems across various industries.\n",
      "Tokens de BERT: ['[CLS]', 'As', 'an', 'AI', '[UNK]', 'Archi', '##tec', '##t', ',', 'I', 'lea', '##d', 'pro', '##ject', '##s', 'invol', '##ving', 'the', 'design', 'and', 'implement', '##ation', 'of', 's', '##cala', '##ble', 'AI', 's', '##ystem', '##s', ',', 'ap', '##pl', '##yi', '##ng', 'ad', '##van', '##ced', 'mach', '##ine', 'lea', '##r', '##ning', 'tech', '##ni', '##ques', 'to', 'sol', '##ve', 'comple', '##x', 'proble', '##ms', 'ac', '##ross', 'vari', '##ou', '##s', 'industri', '##es', '.', '[SEP]']\n",
      "--------------------------------------------------\n",
      "Oración 7:\n",
      "Texto original: My research in NLP explores the intricacies of human language, aiming to enhance natural language understanding and develop applications that improve human-computer interaction.\n",
      "Tokens de BERT: ['[CLS]', 'My', 'rese', '##ar', '##ch', 'in', 'NL', '##P', 'explor', '##es', 'the', 'intri', '##ca', '##cies', 'of', 'human', 'lan', '##gua', '##ge', ',', 'a', '##imi', '##ng', 'to', 'en', '##han', '##ce', 'natural', 'lan', '##gua', '##ge', 'under', '##stan', '##ding', 'and', 'dev', '##elo', '##p', 'ap', '##plica', '##tions', 'that', 'impro', '##ve', 'human', '-', 'comput', '##er', 'interac', '##tion', '.', '[SEP]']\n",
      "--------------------------------------------------\n",
      "Oración 8:\n",
      "Texto original: I have been involved in numerous collaborative initiatives that connect academic research with industry needs, facilitating the transfer of knowledge and technology to create impactful solutions.\n",
      "Tokens de BERT: ['[CLS]', 'I', 'have', 'be', '##en', 'invol', '##ved', 'in', 'numero', '##us', 'col', '##lab', '##ora', '##tive', 'in', '##iti', '##ati', '##ves', 'that', 'con', '##ne', '##ct', 'aca', '##demi', '##c', 'rese', '##ar', '##ch', 'w', '##ith', 'indust', '##ry', 'ne', '##eds', ',', 'facilita', '##ting', 'the', 'transfer', 'of', '[UNK]', 'and', 'tech', '##nol', '##o', '##gy', 'to', 'crea', '##te', 'impac', '##t', '##ful', 'solu', '##tions', '.', '[SEP]']\n",
      "--------------------------------------------------\n",
      "Oración 9:\n",
      "Texto original: My commitment to innovation and excellence has been recognized through various grants and awards, supporting projects that advance the field of AI.\n",
      "Tokens de BERT: ['[CLS]', 'My', 'com', '##mit', '##ment', 'to', 'innov', '##ation', 'and', 'excel', '##len', '##ce', 'has', 'be', '##en', 'reco', '##gni', '##ze', '##d', 'th', '##ro', '##ugh', 'vari', '##ou', '##s', 'gran', '##ts', 'and', '[UNK]', ',', 'sup', '##port', '##ing', 'pro', '##ject', '##s', 'that', 'ad', '##van', '##ce', 'the', 'fiel', '##d', 'of', 'AI', '.', '[SEP]']\n",
      "--------------------------------------------------\n",
      "Oración 10:\n",
      "Texto original: As a Senior Member of the IEEE, I actively participate in international conferences and workshops, contributing to the global discourse on AI advancements and best practices.\n",
      "Tokens de BERT: ['[CLS]', 'As', 'a', 'Sen', '##ior', 'Mem', '##ber', 'of', 'the', 'IE', '##E', '##E', ',', 'I', 'activ', '##ely', 'participa', '##te', 'in', 'interna', '##tional', 'confer', '##ence', '##s', 'and', '[UNK]', ',', 'contrib', '##uti', '##ng', 'to', 'the', 'global', 'disco', '##urs', '##e', 'on', 'AI', 'ad', '##van', '##ce', '##ments', 'and', 'bes', '##t', 'practic', '##es', '.', '[SEP]']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pregunta = \"¿Cuál es mi nombre?\"\n",
    "\n",
    "#Tokenizar el contexto por oraciones usando nltk\n",
    "oraciones_contexto = nltk.sent_tokenize(contexto)\n",
    "\n",
    "# Añadir la pregunta como la primera oración\n",
    "oraciones = [pregunta] + oraciones_contexto\n",
    "\n",
    "# Procesar cada oración por separado\n",
    "for idx, oracion in enumerate(oraciones):\n",
    "    # Codificar la oración sola, utilizando encode_plus para cada oración\n",
    "    encode = tokenizer.encode_plus(oracion, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    \n",
    "    # Obtener los input_ids y convertirlos en tokens\n",
    "    input_ids = encode[\"input_ids\"]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    # Mostrar la oración original y sus tokens\n",
    "    print(f\"Oración {idx + 1}:\")\n",
    "    print(f\"Texto original: {oracion}\")\n",
    "    print(f\"Tokens de BERT: {tokens}\")\n",
    "    print(\"-\" * 50)  # Separador entre oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hawFgMcHZDpe"
   },
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "\n",
    "def pregunta_respuesta(model, contexto, nlp):\n",
    "\n",
    "  # Imprimir contexto\n",
    "  print('Contexto:')\n",
    "  print('-----------------')\n",
    "  print('\\n'.join(wrap(contexto)))\n",
    "\n",
    "  # Loop preguntas-respuestas:\n",
    "  continuar = True\n",
    "  while continuar:\n",
    "    print('\\nPregunta:')\n",
    "    print('-----------------')\n",
    "    pregunta = str(input())\n",
    "\n",
    "    continuar = pregunta!=''\n",
    "\n",
    "    if continuar:\n",
    "      salida = nlp({'question':pregunta, 'context':contexto})\n",
    "      print('\\nRespuesta:')\n",
    "      print('-----------------')\n",
    "      print(salida['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ac7YqpvHd5td"
   },
   "outputs": [],
   "source": [
    "pregunta_respuesta(model, contexto, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJdZuTdTjFff",
    "outputId": "ac8ac5a5-a86e-4031-fb47-eed4a314f377"
   },
   "outputs": [],
   "source": [
    "contexto = \"Avengers: Endgame (Vengadores: Endgame en España)2​ es una película de superhéroes estadounidense de 2019 basada en el equipo de superhéroes Los Vengadores de Marvel Comics. Producida por Marvel Studios y distribuida por Walt Disney Studios Motion Pictures, es la secuela directa de Avengers: Infinity War (2018) y la película número 22 del Universo cinematográfico de Marvel (UCM), y hasta la fecha la más larga. Dirigida por Anthony y Joe Russo y escrita por Christopher Markus y Stephen McFeely, la película cuenta con un reparto coral que incluye a Robert Downey Jr., Chris Evans, Mark Ruffalo, Chris Hemsworth, Scarlett Johansson, Jeremy Renner, Don Cheadle, Paul Rudd, Brie Larson, Karen Gillan, Danai Gurira, Benedict Wong, Jon Favreau, Bradley Cooper, Gwyneth Paltrow y Josh Brolin. En la película, los miembros supervivientes de Los Vengadores y sus aliados intentan revertir el daño causado por Thanos en Infinity War.\"\n",
    "pregunta_respuesta(model, contexto, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
