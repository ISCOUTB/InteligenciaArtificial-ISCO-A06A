{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kls49aGTRHWg"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "uPjGoj2qRjA-",
        "outputId": "3760fc3a-d071-44ba-cd41-229ca0f21a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-796b892d9edc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'We will use the GPU:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;32mraise\u001b[0m  \u001b[0;31m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0m__all__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train=pd.read_csv(\"/content/data/Valence_train_oc_es.csv\", encoding=\"utf8\", sep=\";\")\n",
        "df_test=pd.read_csv(\"/content/data/Valence_test_oc_es.csv\", encoding=\"utf8\", sep=\";\")"
      ],
      "metadata": {
        "id": "O-oDeNxFRlFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ElectraTokenizer, ElectraForSequenceClassification,AdamW\n",
        "import torch\n",
        "tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n",
        "model = ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator',num_labels=2)\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "kjvyTDOQUfRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nueva secci√≥n"
      ],
      "metadata": {
        "id": "lZ7wlsJgKbVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the lists of lyrics and their labels.\n",
        "texts = df_train.Tweet.values\n",
        "labels = df_train.Intensity.values"
      ],
      "metadata": {
        "id": "TYsFs5owXL5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to show length of embedding will be helpful to determine maximum length of comments and padding threshold\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_sentence_embeddings_length(text_list, tokenizer):\n",
        "    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t), text_list))\n",
        "    tokenized_texts_len = list(map(lambda t: len(t), tokenized_texts))\n",
        "    fig, ax = plt.subplots(figsize=(8, 5));\n",
        "    ax.hist(tokenized_texts_len, bins=40);\n",
        "    ax.set_xlabel(\"Length of Comment Embeddings\");\n",
        "    ax.set_ylabel(\"Number of Comments\");\n",
        "    return\n",
        "plot_sentence_embeddings_length(texts, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "5LqZo0tCW4B7",
        "outputId": "3249883e-b956-4ff8-d7ae-4dfd723f66f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices=tokenizer.batch_encode_plus(texts,max_length=64,add_special_tokens=True, return_attention_mask=True,padding=True,truncation=True)\n",
        "\n",
        "input_ids=indices[\"input_ids\"]\n",
        "attention_masks=indices[\"attention_mask\"]"
      ],
      "metadata": {
        "id": "hVtFmQFiXgCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 99% for training and 1% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
        "                                                            random_state=42, test_size=0.2)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=42, test_size=0.2)"
      ],
      "metadata": {
        "id": "2fOEj51JXwih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n",
        "train_masks = torch.tensor(train_masks, dtype=torch.long)\n",
        "validation_masks = torch.tensor(validation_masks, dtype=torch.long)"
      ],
      "metadata": {
        "id": "MERwwug1Zuev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "cG8XZAINZ1gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                  lr = 6e-6, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 5\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "6twkRwgDaNad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "6R8rECthaqah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "6YE6URWbazwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 100 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    labels=b_labels)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the\n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "OFcdSrxobhuC",
        "outputId": "6c9fec7c-b3c2-4f69-eb87-b378a9b2f8df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-42df414f43b4>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/random.py\u001b[0m in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_in_bad_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mcb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mdefault_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_generators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    }
  ]
}